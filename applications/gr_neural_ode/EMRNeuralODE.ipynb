{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750423c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random,OrdinaryDiffEq, DiffEqFlux, Optim, LineSearches, Plots\n",
    "using MatrixFreeNewton\n",
    "gr()\n",
    "include(\"utils.jl\")\n",
    "include(\"models.jl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the experimental parameters\n",
    "mass_ratio = 0.0\n",
    "u0 = Float64[pi, 0.0]\n",
    "datasize = 250\n",
    "tspan = (0.0f0, 24.0f4)\n",
    "tsteps = range(tspan[1], tspan[2], length = datasize)\n",
    "dt_data = tsteps[2] - tsteps[1]\n",
    "dt = 100.0\n",
    "model_params = [100.0, 1.0, 0.5] # p, M, e\n",
    "\n",
    "# Generate waveform data\n",
    "prob = ODEProblem(RelativisticOrbitModel, u0, tspan, model_params)\n",
    "soln = Array(solve(prob, RK4(), saveat = tsteps, dt = dt, adaptive=false))\n",
    "waveform = compute_waveform(dt_data, soln, mass_ratio, model_params)[1]\n",
    "\n",
    "plt = plot(tsteps, waveform,\n",
    "           markershape=:circle, markeralpha = 0.25,\n",
    "           linewidth = 2, alpha = 0.5,\n",
    "           label=\"waveform data\")\n",
    "\n",
    "## Define neural network model\n",
    "n_hidden = 16\n",
    "NN = FastChain((x, p) -> [cos(x[1])],\n",
    "                FastDense(1, n_hidden, cos),\n",
    "                FastDense(n_hidden, n_hidden, cos),\n",
    "                FastDense(n_hidden, 2))\n",
    "NN_params = initial_params(NN) .* 0\n",
    "\n",
    "println(\"Size of nn parameters: \",size(NN_params))\n",
    "\n",
    "function ODE_model(u, NN_params, t)\n",
    "    du = AbstractNNOrbitModel(u, model_params, t, NN=NN, NN_params=NN_params)\n",
    "    return du\n",
    "end\n",
    "\n",
    "prob_nn = ODEProblem(ODE_model, u0, tspan, NN_params)\n",
    "soln_nn = Array(solve(prob_nn, RK4(), u0 = u0, p = NN_params, saveat = tsteps, dt = dt, adaptive=false))\n",
    "waveform_nn = compute_waveform(dt_data, soln_nn, mass_ratio, model_params)[1]\n",
    "\n",
    "plot!(plt, tsteps, waveform_nn,\n",
    "           markershape=:circle, markeralpha = 0.25,\n",
    "           linewidth = 2, alpha = 0.5,\n",
    "           label=\"waveform NN\")\n",
    "display(plt)\n",
    "\n",
    "## Define objective function\n",
    "function loss(NN_params)\n",
    "    # Here is where to try different adjoints\n",
    "    pred = Array(solve(prob_nn, RK4(), u0 = u0, p = NN_params, saveat = tsteps, dt = dt, adaptive=false))\n",
    "    pred_waveform = compute_waveform(dt_data, pred, mass_ratio, model_params)[1]\n",
    "    loss = ( sum(abs2, waveform .- pred_waveform) )\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6169fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 50\n",
    "lrsfn_rank = 40\n",
    "\n",
    "# Allocate logger dictionaries for different draws\n",
    "loggers_gd = Dict()\n",
    "loggers_lrsfn = Dict()\n",
    "# \n",
    "loggers_bfgs = Dict()\n",
    "struct BFGSLogger\n",
    "    alpha::Float64\n",
    "    losses::AbstractArray{Float64}\n",
    "end\n",
    "\n",
    "NN_params = initial_params(NN) .* 0\n",
    "\n",
    "\n",
    "for draw in 2:5\n",
    "    println(\"Running for draw \",draw)\n",
    "    # 1e-3 worked save for one draw\n",
    "    random_state = Random.MersenneTwister(draw)\n",
    "    w_0 = copy(NN_params + 1e-3*randn(random_state,eltype(NN_params), size(NN_params)))\n",
    "\n",
    "#     println(\"Now for gradient descent \")\n",
    "#     w_star_gd,logger_gd = gradientDescent(loss,w_0,alpha = 1e-5, iterations = iterations,printing_frequency = 1)\n",
    "#     loggers_gd[draw] = logger_gd\n",
    "    \n",
    "    \n",
    "    println(\"Now for low rank SFNewton with reduced Hessian with LRSFN rank = \",lrsfn_rank)\n",
    "    w_star_lrsfn,logger_lrsfn = lowRankSaddleFreeNewton(loss,w_0,rank=lrsfn_rank,printing_frequency = 1,\n",
    "                                        alpha = 0.1, hessian = \"matrix_free\",iterations = iterations)\n",
    "\n",
    "    loggers_lrsfn[draw] = logger_lrsfn\n",
    "    \n",
    "    # BFGS\n",
    "    println(\"Now for BFGS\")\n",
    "    global losses_bfgs = zeros(0)\n",
    "    global iter = 0\n",
    "    cb = function(p,l)\n",
    "        global iter += 1 \n",
    "        println(\"Loss at iter \",iter,\" = \",l)\n",
    "        append!(losses_bfgs,l)\n",
    "        return false\n",
    "    end\n",
    "    alpha_bfgs = 0.05\n",
    "    res = DiffEqFlux.sciml_train(loss, w_0, BFGS(initial_stepnorm=alpha_bfgs),\n",
    "                                            maxiters = iterations,progress = true,cb = cb)\n",
    "    println(\"Losses for BFGS\", losses_bfgs)\n",
    "    logger_bfgs = BFGSLogger(alpha_bfgs,losses_bfgs)\n",
    "    loggers_bfgs[draw] = logger_bfgs\n",
    "    \n",
    "end\n",
    "println(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using NPZ, Statistics\n",
    "# Save data for post-processing\n",
    "data_dir = \"emr_data/\"\n",
    "if ~isdir(data_dir)\n",
    "    mkdir(data_dir)\n",
    "end\n",
    "problem_name = \"emr\"\n",
    "optimizers = [\"gd\",\"lrsfn\",\"bfgs\"]\n",
    "logger_dicts = [loggers_gd,loggers_lrsfn,loggers_bfgs]\n",
    "\n",
    "for (optimizer,logger_dict) in zip(optimizers,logger_dicts)\n",
    "    println(\"optimizer = \",optimizer)\n",
    "    opt_losses = zeros(0)\n",
    "    for (draw,logger) in logger_dict\n",
    "        name = problem_name*optimizer*\"_alpha_\"*string(logger.alpha)*\"_\"*string(draw)\n",
    "        if optimizer == \"lrsfn\"\n",
    "            name *=\"rank_\"*string(logger.rank)\n",
    "        end\n",
    "        println(\"name = \",name)\n",
    "        # Save losses\n",
    "        npzwrite(data_dir*name*\"_losses.npy\",logger.losses)\n",
    "        min_loss = minimum(logger.losses)\n",
    "        append!(opt_losses,min_loss)\n",
    "        # If sfn save spectrum:\n",
    "        if optimizer in [\"sfn\",\"lrsfn\"]\n",
    "\n",
    "            npzwrite(data_dir*name*\"_spectra.npy\",logger.spectra)\n",
    "        end\n",
    "        # If csgd save alphas\n",
    "        if optimizer in [\"csgd\"]\n",
    "            npzwrite(data_dir*name*\"_alphas.npy\",logger.alphas)\n",
    "        end\n",
    "\n",
    "    end\n",
    "    println(\"Min min loss = \",minimum(opt_losses))\n",
    "    println(\"Avg min loss = \",Statistics.mean(opt_losses))\n",
    "    println(\"Std min loss = \",Statistics.std(opt_losses,corrected = false))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d45356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
